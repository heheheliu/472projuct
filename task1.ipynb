{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "#sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 Plot distribution of the instances in each class and save the graphic in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "business\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "510"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numOffolds = len(os.listdir('BBC/'))\n",
    "print(numOffolds)\n",
    "all_folds = os.listdir('BBC/')\n",
    "folder_path = all_folds[0]\n",
    "print(folder_path)\n",
    "\n",
    "len([x for x in os.listdir('BBC/'+folder_path)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comput No.1 folder.\n",
      "No.1 has 510 files.\n",
      "comput No.2 folder.\n",
      "No.2 has 386 files.\n",
      "comput No.3 folder.\n",
      "No.3 has 417 files.\n",
      "comput No.4 folder.\n",
      "No.4 has 511 files.\n",
      "comput No.5 folder.\n",
      "No.5 has 401 files.\n"
     ]
    }
   ],
   "source": [
    "data_need = []\n",
    "data_x = []\n",
    "data_y = []\n",
    "\n",
    "\n",
    "path = 'BBC/'\n",
    "all_folds = os.listdir(path)\n",
    "\n",
    "for i in range (len(all_folds)):\n",
    "    data_x.append(all_folds[i])\n",
    "    print(f'comput No.{i+1} folder.')\n",
    "    folder_path = all_folds[i]\n",
    "    NumOfFiles = len([x for x in os.listdir(path + folder_path)])\n",
    "    data_y.append(NumOfFiles)\n",
    "    print(f'No.{i+1} has {NumOfFiles} files.')\n",
    "    table = pd.DataFrame({'folder_id': all_folds[i],'file_num':[NumOfFiles]})\n",
    "    data_need.append(table)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       folder_id  file_num\n",
      "0       business       510\n",
      "0  entertainment       386\n",
      "0       politics       417\n",
      "0          sport       511\n",
      "0           tech       401\n"
     ]
    }
   ],
   "source": [
    "data_final = pd.concat(data_need, axis=0)\n",
    "print(data_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['business' 'entertainment' 'politics' 'sport' 'tech']\n",
      "[510 386 417 511 401]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_np = np.array(data_x)\n",
    "print(x_np)\n",
    "y_np = np.array(data_y)\n",
    "print(y_np)\n",
    "plt.title(\"Distribution of the instances in each class\")\n",
    "plt.xlabel(\"class name\")\n",
    "plt.ylabel(\"number of instances\")\n",
    "plt.bar(x_np,y_np)\n",
    "plt.savefig('BBC-Distribution.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 4 2 ... 1 1 3]\n",
      "['business', 'entertainment', 'politics', 'sport', 'tech']\n"
     ]
    }
   ],
   "source": [
    "# use load_files to read the file structure and assign the category name to each file\n",
    "corpus_data = load_files(\"BBC\",encoding=\"latin1\")\n",
    "corpus_data_data = corpus_data.data\n",
    "# print(corpus_data_data)\n",
    "corpus_category = corpus_data.target\n",
    "print(corpus_category)\n",
    "corpus_category_names = corpus_data.target_names\n",
    "print(corpus_category_names)\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 Pre-process the dataset to have the features ready to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29421\n",
      "(2225, 29421)\n",
      "836357\n"
     ]
    }
   ],
   "source": [
    "#create a Vectorizer Object\n",
    "vectorizer = CountVectorizer()\n",
    "#tokenize and build vocab\n",
    "vectorizer.fit(corpus_data_data)\n",
    "#summarize\n",
    "vocabulary_corpus = vectorizer.get_feature_names()\n",
    "# print(vocabulary_corpus)\n",
    "print(len(vectorizer.get_feature_names()))\n",
    "\n",
    "\n",
    "# Encode the dataset to document-term matrix\n",
    "vector = vectorizer.transform(corpus_data_data)\n",
    "print(vector.shape)\n",
    "print(vector.toarray().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bird', 'cat', 'dog', 'fish']\n",
      "[[0 1 1 1]\n",
      " [0 2 1 0]\n",
      " [1 0 0 1]\n",
      " [1 0 0 0]]\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "texts=[\"dog cat fish\",\"dog cat cat\",\"fish bird\", 'bird']\n",
    "cv = CountVectorizer()\n",
    "cv_fit=cv.fit_transform(texts)\n",
    "\n",
    "print(cv.get_feature_names())\n",
    "print(cv_fit.toarray())\n",
    "print(cv_fit.toarray().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 Split the dataset int 80% for training 20% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into 80% for training and 20% for testing\n",
    "X = corpus_data_data\n",
    "y = corpus_category\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, train_size=0.8, random_state= None)\n",
    "# print(X_train)\n",
    "# print(y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26754\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0001</th>\n",
       "      <th>000bn</th>\n",
       "      <th>000m</th>\n",
       "      <th>000th</th>\n",
       "      <th>001st</th>\n",
       "      <th>004</th>\n",
       "      <th>0051</th>\n",
       "      <th>007</th>\n",
       "      <th>...</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zooms</th>\n",
       "      <th>zooropa</th>\n",
       "      <th>zornotza</th>\n",
       "      <th>zorro</th>\n",
       "      <th>zubair</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zvyagintsev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1775</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1776</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1777</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1778</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1779</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1780 rows × 26754 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     00 000 0001 000bn 000m 000th 001st 004 0051 007  ... zone zones zoom  \\\n",
       "0     0   0    0     0    0     0     0   0    0   0  ...    0     0    0   \n",
       "1     0   1    0     0    0     0     0   0    0   0  ...    0     0    0   \n",
       "2     0   4    0     0    0     0     0   0    0   0  ...    0     0    0   \n",
       "3     0   0    0     0    0     0     0   0    0   0  ...    0     0    0   \n",
       "4     0   0    0     0    0     0     0   0    0   0  ...    0     0    0   \n",
       "...  ..  ..  ...   ...  ...   ...   ...  ..  ...  ..  ...  ...   ...  ...   \n",
       "1775  0   0    0     0    0     0     0   0    0   0  ...    0     0    0   \n",
       "1776  0   0    0     0    0     0     0   0    0   0  ...    0     0    0   \n",
       "1777  0   0    0     0    0     0     0   0    0   0  ...    0     0    0   \n",
       "1778  0   0    0     0    0     0     0   0    0   0  ...    0     0    0   \n",
       "1779  0   2    0     0    0     0     0   0    0   0  ...    0     0    0   \n",
       "\n",
       "     zooms zooropa zornotza zorro zubair zurich zvyagintsev  \n",
       "0        0       0        0     0      0      0           0  \n",
       "1        0       0        0     0      0      0           0  \n",
       "2        0       0        0     0      0      0           0  \n",
       "3        0       0        0     0      0      0           0  \n",
       "4        0       0        0     0      0      0           0  \n",
       "...    ...     ...      ...   ...    ...    ...         ...  \n",
       "1775     0       0        0     0      0      0           0  \n",
       "1776     0       0        0     0      0      0           0  \n",
       "1777     0       0        0     0      0      0           0  \n",
       "1778     0       0        0     0      0      0           0  \n",
       "1779     0       0        0     0      0      0           0  \n",
       "\n",
       "[1780 rows x 26754 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# countVectorizer\n",
    "vec = CountVectorizer()\n",
    "# fit the vectorizer on the training dataset\n",
    "vec.fit(X_train)\n",
    "print(len(vec.get_feature_names()))\n",
    "vocab = vec.get_feature_names()\n",
    "# print(vocab)\n",
    "X_train_transformed = vec.transform(X_train)\n",
    "# print(X_train_transformed)\n",
    "# convert training set to document-term matrix\n",
    "X_train_feat = X_train_transformed.toarray()\n",
    "# convert to sparse matrix for readability\n",
    "pd.DataFrame(X_train_feat,columns= [vec.get_feature_names()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0001</th>\n",
       "      <th>000bn</th>\n",
       "      <th>000m</th>\n",
       "      <th>000th</th>\n",
       "      <th>001st</th>\n",
       "      <th>004</th>\n",
       "      <th>0051</th>\n",
       "      <th>007</th>\n",
       "      <th>...</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zooms</th>\n",
       "      <th>zooropa</th>\n",
       "      <th>zornotza</th>\n",
       "      <th>zorro</th>\n",
       "      <th>zubair</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zvyagintsev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>445 rows × 26754 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    00 000 0001 000bn 000m 000th 001st 004 0051 007  ... zone zones zoom  \\\n",
       "0    0   0    0     0    0     0     0   0    0   0  ...    0     0    0   \n",
       "1    0   0    0     0    0     0     0   0    0   0  ...    0     0    0   \n",
       "2    0   2    0     0    0     0     0   0    0   0  ...    0     0    0   \n",
       "3    0   0    0     0    0     0     0   0    0   0  ...    0     0    0   \n",
       "4    0   2    0     0    0     0     0   0    0   0  ...    0     0    0   \n",
       "..  ..  ..  ...   ...  ...   ...   ...  ..  ...  ..  ...  ...   ...  ...   \n",
       "440  0   0    0     0    0     0     0   0    0   0  ...    0     0    0   \n",
       "441  0   0    0     0    0     0     0   0    0   0  ...    0     0    0   \n",
       "442  0   0    0     0    0     0     0   0    0   0  ...    0     0    0   \n",
       "443  0   0    0     0    0     0     0   0    0   0  ...    0     0    0   \n",
       "444  0   0    0     0    1     0     0   0    0   0  ...    0     0    0   \n",
       "\n",
       "    zooms zooropa zornotza zorro zubair zurich zvyagintsev  \n",
       "0       0       0        0     0      0      0           0  \n",
       "1       0       0        0     0      0      0           0  \n",
       "2       0       0        0     0      0      0           0  \n",
       "3       0       0        0     0      0      0           0  \n",
       "4       0       0        0     0      0      0           0  \n",
       "..    ...     ...      ...   ...    ...    ...         ...  \n",
       "440     0       0        0     0      0      0           0  \n",
       "441     0       0        0     0      0      0           0  \n",
       "442     0       0        0     0      0      0           0  \n",
       "443     0       0        0     0      0      0           0  \n",
       "444     0       0        0     0      0      0           0  \n",
       "\n",
       "[445 rows x 26754 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for test data\n",
    "X_test_transformed = vec.transform(X_test)\n",
    "# X_test_transformed\n",
    "# print(X_test_transformed)\n",
    "# convert testing set to document-term matrix\n",
    "X_test_feat = X_test_transformed.toarray()\n",
    "# convert to sparse matrix for readability\n",
    "pd.DataFrame(X_test_feat, columns= [vec.get_feature_names()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 train a multinomial Naive Bayes Classifier on the training set using the default parameters and evaluate it on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('bbc-performance.txt','w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multinomial Naive Bayes\n",
    "print(\"--------------------------------MultinomialNB default values, try 1-----------------------------------\",file=f)\n",
    "nb = MultinomialNB(alpha=1)\n",
    "# fit the training set using the default parameters and evaluate it on the test set\n",
    "nb.fit(X_train_transformed, y_train)\n",
    "\n",
    "# predict, Perform classification on an array of test vectors X.\n",
    "y_pred_class = nb.predict(X_test_transformed)\n",
    "# predict probabilities, Return probability estimates for the test vector X.\n",
    "y_pred_proba = nb.predict_proba(X_test_transformed)\n",
    "\n",
    "# b) confusion matrix\n",
    "from sklearn import metrics\n",
    "confusion = metrics.confusion_matrix(y_test, y_pred_class)\n",
    "print(\"(b) the confusion matrix: \", file=f)\n",
    "print(confusion, file=f)\n",
    "\n",
    "\n",
    "\n",
    "# c) the precision, recall and F1-measure for each class\n",
    "print(\"(c) the precision, recall and F1-measure for each class of the test set : \", file=f)\n",
    "print(metrics.classification_report(y_test, y_pred_class), file=f)\n",
    "\n",
    "\n",
    "\n",
    "# d) the accuracy, macro-average F1 and weighted-average F1 of the model\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred_class)\n",
    "print(\"(d) the accuracy, macro-average F1 and weighted-average F1 of the model: \", file=f)\n",
    "print(\"Accuracy score of the test set is : \" + str(accuracy),file=f)\n",
    "\n",
    "macro_avg_F1 = metrics.f1_score(y_test, y_pred_class, average='macro')\n",
    "print(\"Macro average F1 of the test set is : \" + str(macro_avg_F1),file=f)\n",
    "weighted_avg_F1 = metrics.f1_score(y_test, y_pred_class, average='weighted')\n",
    "print(\"Weighted average F1 of the test set is : \" + str(weighted_avg_F1),file=f)\n",
    "\n",
    "# index = nb.predict(vec.transform(['Need to restart economy but with caution: Yogi Adityanath at E-Agenda AajTak']))\n",
    "\n",
    "# def type_check(i):\n",
    "#     if i == 0:\n",
    "#         print('business')\n",
    "#     elif i == 1:\n",
    "#         print('entertainment')\n",
    "#     elif i == 2:\n",
    "#         print('politics')\n",
    "#     elif i == 3:\n",
    "#         print('sport')\n",
    "#     elif i == 4:\n",
    "#         print('tech')\n",
    "# type_check(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e) the prior probability of each class\n",
    "print(\"(e) the prior probability of each class : \",file=f)\n",
    "\n",
    "sum = np.sum(y_np)\n",
    "print(sum,file=f)\n",
    "index = 0\n",
    "for i in x_np:\n",
    "    prior = y_np[index]/sum\n",
    "    index = index + 1\n",
    "    print(i + ': ' + str(prior), file=f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f) the size of the vocabulary(the number of different words)\n",
    "print(\"(f) the size of the vocabulary(the number of different words\",file=f)\n",
    "print(len(vocabulary_corpus),file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g) the number of word-tokens in each class\n",
    "print(\"(g) the number of word-tokens in each class: \",file=f)\n",
    "for class_name in corpus_category_names:\n",
    "    # print(class_name)\n",
    "    class_data = load_files(\"BBC\", categories=class_name, encoding='latin1')\n",
    "    class_data_data = class_data.data\n",
    "    class_vec = CountVectorizer()\n",
    "    class_array = class_vec.fit_transform(class_data_data).toarray()\n",
    "    sum_class = class_array.sum()\n",
    "\n",
    "    \n",
    "    print(\"The number of word-tokens in \" + class_name + \": \" + str(sum_class),file=f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h) the number of word-tokens in the entire corpus\n",
    "print('(h) the number of word-tokens in the entire corpus: ',file=f)\n",
    "sum_total = vector.toarray().sum() \n",
    "print(\"Total word-tokens in entire corpus: \" + str(sum_total),file=f)\n",
    "\n",
    "# j) the number and percentage of words with a frequency of 1 in the entire corpus\n",
    "print('(j) the number and percentage of words with a frequency of 1 in the entire corpus: ',file=f)\n",
    "count_word = 0\n",
    "for word in (vector.toarray().sum(axis = 0)):\n",
    "    if word == 1:\n",
    "        count_word = count_word + 1\n",
    "print(\"The number of words with a frequency of 1 in entire corpus: \" + str(count_word),file=f)\n",
    "print(\"The percentage of words with a frequency of 1 in entire corpus: \" + str(count_word/sum_total),file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i) the number and percentage of words with freuency of zero in each class\n",
    "print('(i) the number and percentage of words with freuency of zero in each class: ',file=f)\n",
    "for class_name in corpus_category_names:\n",
    "    # print(class_name)\n",
    "    class_data = load_files(\"BBC\", categories=class_name, encoding='latin1')\n",
    "    class_data_data = class_data.data\n",
    "    class_vec = CountVectorizer()\n",
    "    class_vec.fit(class_data_data)\n",
    "    num_feat_word = len(class_vec.get_feature_names())\n",
    "    num_zero = len(vocabulary_corpus)-num_feat_word\n",
    "    print(\"The number of words with frequency of zero in \" + class_name + \": \"+ str(num_zero),file=f)  # frequency , which divide which\n",
    "    print(\"The percentage of words with frequency of zero in \" + class_name + \": \"+ str(num_zero/len(vocabulary_corpus)),file=f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "counts = pd.DataFrame(vector.toarray(), columns=vectorizer.get_feature_names())\n",
    "print(\"(k) 2 favoirite words and their log-prob\",file=f)\n",
    "# counts\n",
    "# show the top 10 most common words\n",
    "# print(counts.T.sort_values(by=0, ascending=False).head(10))\n",
    "import math\n",
    "count_love = counts['love'].sum()\n",
    "print(\"frequency of 'love': \" + str(count_love),file=f)\n",
    "log_prob_love = math.log(count_love/sum_total)\n",
    "print(\"the log-prob of the 'love' favorite word is: \" + str(log_prob_love),file=f)\n",
    "count_key = counts['key'].sum()\n",
    "print(\"frequency of 'key': \" + str(count_key),file=f)\n",
    "log_prob_key = math.log(count_key/sum_total)\n",
    "print(\"the log-prob of the 'key' favorite word is: \" + str(log_prob_key),file=f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multinomial Naive Bayes\n",
    "\n",
    "print(\"--------------------------------MultinomialNB default value, try 2-----------------------------------\",file=f)\n",
    "nb = MultinomialNB(alpha=1)\n",
    "# fit the training set using the default parameters and evaluate it on the test set\n",
    "nb.fit(X_train_transformed, y_train)\n",
    "\n",
    "# predict, Perform classification on an array of test vectors X.\n",
    "y_pred_class = nb.predict(X_test_transformed)\n",
    "# predict probabilities, Return probability estimates for the test vector X.\n",
    "y_pred_proba = nb.predict_proba(X_test_transformed)\n",
    "\n",
    "# b) confusion matrix\n",
    "from sklearn import metrics\n",
    "confusion = metrics.confusion_matrix(y_test, y_pred_class)\n",
    "print(\"(b) the confusion matrix: \", file=f)\n",
    "print(confusion, file=f)\n",
    "\n",
    "\n",
    "\n",
    "# c) the precision, recall and F1-measure for each class\n",
    "print(\"(c) the precision, recall and F1-measure for each class of the test set : \",file=f)\n",
    "print(metrics.classification_report(y_test, y_pred_class),file=f)\n",
    "\n",
    "\n",
    "\n",
    "# d) the accuracy, macro-average F1 and weighted-average F1 of the model\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred_class)\n",
    "print(\"(d) the accuracy, macro-average F1 and weighted-average F1 of the model: \",file=f)\n",
    "print(\"Accuracy score of the test set is : \" + str(accuracy),file=f)\n",
    "\n",
    "macro_avg_F1 = metrics.f1_score(y_test, y_pred_class, average='macro')\n",
    "print(\"Macro average F1 of the test set is : \" + str(macro_avg_F1),file=f)\n",
    "weighted_avg_F1 = metrics.f1_score(y_test, y_pred_class, average='weighted')\n",
    "print(\"Weighted average F1 of the test set is : \" + str(weighted_avg_F1),file=f)\n",
    "# e) the prior probability of each class\n",
    "print(\"(e) the prior probability of each class : \",file=f)\n",
    "\n",
    "sum = np.sum(y_np)\n",
    "print(sum,file=f)\n",
    "index = 0\n",
    "for i in x_np:\n",
    "    prior = y_np[index]/sum\n",
    "    index = index + 1\n",
    "    print(i + ': ' + str(prior), file=f)\n",
    "\n",
    "# f) the size of the vocabulary(the number of different words)\n",
    "print(\"(f) the size of the vocabulary(the number of different words\",file=f)\n",
    "print(len(vocabulary_corpus),file=f)\n",
    "# g) the number of word-tokens in each class\n",
    "print(\"(g) the number of word-tokens in each class: \",file=f)\n",
    "for class_name in corpus_category_names:\n",
    "    # print(class_name)\n",
    "    class_data = load_files(\"BBC\", categories=class_name, encoding='latin1')\n",
    "    class_data_data = class_data.data\n",
    "    class_vec = CountVectorizer()\n",
    "    class_array = class_vec.fit_transform(class_data_data).toarray()\n",
    "    sum_class = class_array.sum()\n",
    "\n",
    "    \n",
    "    print(\"The number of word-tokens in \" + class_name + \": \" + str(sum_class),file=f)\n",
    "    \n",
    "# h) the number of word-tokens in the entire corpus\n",
    "print('(h) the number of word-tokens in the entire corpus: ',file=f)\n",
    "sum_total = vector.toarray().sum() \n",
    "print(\"Total word-tokens in entire corpus: \" + str(sum_total),file=f)\n",
    "\n",
    "# i) the number and percentage of words with freuency of zero in each class\n",
    "print('(i) the number and percentage of words with freuency of zero in each class: ',file=f)\n",
    "for class_name in corpus_category_names:\n",
    "    # print(class_name)\n",
    "    class_data = load_files(\"BBC\", categories=class_name, encoding='latin1')\n",
    "    class_data_data = class_data.data\n",
    "    class_vec = CountVectorizer()\n",
    "    class_vec.fit(class_data_data)\n",
    "    num_feat_word = len(class_vec.get_feature_names())\n",
    "    num_zero = len(vocabulary_corpus)-num_feat_word\n",
    "    print(\"The number of words with frequency of zero in \" + class_name + \": \"+ str(num_zero),file=f)  # frequency , which divide which\n",
    "    print(\"The percentage of words with frequency of zero in \" + class_name + \": \"+ str(num_zero/len(vocabulary_corpus)),file=f)\n",
    "\n",
    "# j) the number and percentage of words with a frequency of 1 in the entire corpus\n",
    "print('(j) the number and percentage of words with a frequency of 1 in the entire corpus: ',file=f)\n",
    "count_word = 0\n",
    "for word in (vector.toarray().sum(axis = 0)):\n",
    "    if word == 1:\n",
    "        count_word = count_word + 1\n",
    "print(\"The number of words with a frequency of 1 in entire corpus: \" + str(count_word),file=f)\n",
    "print(\"The percentage of words with a frequency of 1 in entire corpus: \" + str(count_word/sum_total),file=f)\n",
    "\n",
    "\n",
    "counts = pd.DataFrame(vector.toarray(), columns=vectorizer.get_feature_names())\n",
    "print(\"(k) 2 favoirite words and their log-prob\",file=f)\n",
    "# counts\n",
    "# show the top 10 most common words\n",
    "# print(counts.T.sort_values(by=0, ascending=False).head(10))\n",
    "import math\n",
    "count_love = counts['love'].sum()\n",
    "print(\"frequency of 'love': \" + str(count_love),file=f)\n",
    "log_prob_love = math.log(count_love/sum_total)\n",
    "print(\"the log-prob of the 'love' favorite word is: \" + str(log_prob_love),file=f)\n",
    "count_key = counts['key'].sum()\n",
    "print(\"frequency of 'key': \" + str(count_key),file=f)\n",
    "log_prob_key = math.log(count_key/sum_total)\n",
    "print(\"the log-prob of the 'key' favorite word is: \" + str(log_prob_key),file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. smooth value to 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multinomial Naive Bayes\n",
    "\n",
    "print(\"--------------------------------MultinomialNB smooth valve to 0.0001-----------------------------------\",file=f)\n",
    "nb = MultinomialNB(alpha=0.0001)\n",
    "# fit the training set using the default parameters and evaluate it on the test set\n",
    "nb.fit(X_train_transformed, y_train)\n",
    "\n",
    "# predict, Perform classification on an array of test vectors X.\n",
    "y_pred_class = nb.predict(X_test_transformed)\n",
    "# predict probabilities, Return probability estimates for the test vector X.\n",
    "y_pred_proba = nb.predict_proba(X_test_transformed)\n",
    "\n",
    "# b) confusion matrix\n",
    "from sklearn import metrics\n",
    "confusion = metrics.confusion_matrix(y_test, y_pred_class)\n",
    "print(\"(b) the confusion matrix: \", file=f)\n",
    "print(confusion, file=f)\n",
    "\n",
    "\n",
    "\n",
    "# c) the precision, recall and F1-measure for each class\n",
    "print(\"(c) the precision, recall and F1-measure for each class of the test set : \",file=f)\n",
    "print(metrics.classification_report(y_test, y_pred_class),file=f)\n",
    "\n",
    "\n",
    "\n",
    "# d) the accuracy, macro-average F1 and weighted-average F1 of the model\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred_class)\n",
    "print(\"(d) the accuracy, macro-average F1 and weighted-average F1 of the model: \",file=f)\n",
    "print(\"Accuracy score of the test set is : \" + str(accuracy),file=f)\n",
    "\n",
    "macro_avg_F1 = metrics.f1_score(y_test, y_pred_class, average='macro')\n",
    "print(\"Macro average F1 of the test set is : \" + str(macro_avg_F1),file=f)\n",
    "weighted_avg_F1 = metrics.f1_score(y_test, y_pred_class, average='weighted')\n",
    "print(\"Weighted average F1 of the test set is : \" + str(weighted_avg_F1),file=f)\n",
    "# e) the prior probability of each class\n",
    "print(\"(e) the prior probability of each class : \",file=f)\n",
    "\n",
    "sum = np.sum(y_np)\n",
    "print(sum,file=f)\n",
    "index = 0\n",
    "for i in x_np:\n",
    "    prior = y_np[index]/sum\n",
    "    index = index + 1\n",
    "    print(i + ': ' + str(prior), file=f)\n",
    "\n",
    "# f) the size of the vocabulary(the number of different words)\n",
    "print(\"(f) the size of the vocabulary(the number of different words\",file=f)\n",
    "print(len(vocabulary_corpus),file=f)\n",
    "# g) the number of word-tokens in each class\n",
    "print(\"(g) the number of word-tokens in each class: \",file=f)\n",
    "for class_name in corpus_category_names:\n",
    "    # print(class_name)\n",
    "    class_data = load_files(\"BBC\", categories=class_name, encoding='latin1')\n",
    "    class_data_data = class_data.data\n",
    "    class_vec = CountVectorizer()\n",
    "    class_array = class_vec.fit_transform(class_data_data).toarray()\n",
    "    sum_class = class_array.sum()\n",
    "\n",
    "    \n",
    "    print(\"The number of word-tokens in \" + class_name + \": \" + str(sum_class),file=f)\n",
    "    \n",
    "# h) the number of word-tokens in the entire corpus\n",
    "print('(h) the number of word-tokens in the entire corpus: ',file=f)\n",
    "sum_total = vector.toarray().sum() \n",
    "print(\"Total word-tokens in entire corpus: \" + str(sum_total),file=f)\n",
    "\n",
    "# i) the number and percentage of words with freuency of zero in each class\n",
    "print('(i) the number and percentage of words with freuency of zero in each class: ',file=f)\n",
    "for class_name in corpus_category_names:\n",
    "    # print(class_name)\n",
    "    class_data = load_files(\"BBC\", categories=class_name, encoding='latin1')\n",
    "    class_data_data = class_data.data\n",
    "    class_vec = CountVectorizer()\n",
    "    class_vec.fit(class_data_data)\n",
    "    num_feat_word = len(class_vec.get_feature_names())\n",
    "    num_zero = len(vocabulary_corpus)-num_feat_word\n",
    "    print(\"The number of words with frequency of zero in \" + class_name + \": \"+ str(num_zero),file=f)  # frequency , which divide which\n",
    "    print(\"The percentage of words with frequency of zero in \" + class_name + \": \"+ str(num_zero/len(vocabulary_corpus)),file=f)\n",
    "\n",
    "# j) the number and percentage of words with a frequency of 1 in the entire corpus\n",
    "print('(j) the number and percentage of words with a frequency of 1 in the entire corpus: ',file=f)\n",
    "count_word = 0\n",
    "for word in (vector.toarray().sum(axis = 0)):\n",
    "    if word == 1:\n",
    "        count_word = count_word + 1\n",
    "print(\"The number of words with a frequency of 1 in entire corpus: \" + str(count_word),file=f)\n",
    "print(\"The percentage of words with a frequency of 1 in entire corpus: \" + str(count_word/sum_total),file=f)\n",
    "\n",
    "\n",
    "counts = pd.DataFrame(vector.toarray(), columns=vectorizer.get_feature_names())\n",
    "print(\"(k) 2 favoirite words and their log-prob\",file=f)\n",
    "# counts\n",
    "# show the top 10 most common words\n",
    "# print(counts.T.sort_values(by=0, ascending=False).head(10))\n",
    "import math\n",
    "count_love = counts['love'].sum()\n",
    "print(\"frequency of 'love': \" + str(count_love),file=f)\n",
    "log_prob_love = math.log(count_love/sum_total)\n",
    "print(\"the log-prob of the 'love' favorite word is: \" + str(log_prob_love),file=f)\n",
    "count_key = counts['key'].sum()\n",
    "print(\"frequency of 'key': \" + str(count_key),file=f)\n",
    "log_prob_key = math.log(count_key/sum_total)\n",
    "print(\"the log-prob of the 'key' favorite word is: \" + str(log_prob_key),file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Redo steps 6 and 7, but this time change the smooth value to 0.9. Appendthe results at the end of bbc-performance.tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multinomial Naive Bayes\n",
    "\n",
    "print(\"--------------------------------MultinomialNB smooth 0.9-----------------------------------\",file=f)\n",
    "nb = MultinomialNB(alpha=0.9)\n",
    "# fit the training set using the default parameters and evaluate it on the test set\n",
    "nb.fit(X_train_transformed, y_train)\n",
    "\n",
    "# predict, Perform classification on an array of test vectors X.\n",
    "y_pred_class = nb.predict(X_test_transformed)\n",
    "# predict probabilities, Return probability estimates for the test vector X.\n",
    "y_pred_proba = nb.predict_proba(X_test_transformed)\n",
    "\n",
    "# b) confusion matrix\n",
    "from sklearn import metrics\n",
    "confusion = metrics.confusion_matrix(y_test, y_pred_class)\n",
    "print(\"(b) the confusion matrix: \", file=f)\n",
    "print(confusion, file=f)\n",
    "\n",
    "\n",
    "\n",
    "# c) the precision, recall and F1-measure for each class\n",
    "print(\"(c) the precision, recall and F1-measure for each class of the test set : \",file=f)\n",
    "print(metrics.classification_report(y_test, y_pred_class),file=f)\n",
    "\n",
    "\n",
    "\n",
    "# d) the accuracy, macro-average F1 and weighted-average F1 of the model\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred_class)\n",
    "print(\"(d) the accuracy, macro-average F1 and weighted-average F1 of the model: \",file=f)\n",
    "print(\"Accuracy score of the test set is : \" + str(accuracy),file=f)\n",
    "\n",
    "macro_avg_F1 = metrics.f1_score(y_test, y_pred_class, average='macro')\n",
    "print(\"Macro average F1 of the test set is : \" + str(macro_avg_F1),file=f)\n",
    "weighted_avg_F1 = metrics.f1_score(y_test, y_pred_class, average='weighted')\n",
    "print(\"Weighted average F1 of the test set is : \" + str(weighted_avg_F1),file=f)\n",
    "# e) the prior probability of each class\n",
    "print(\"(e) the prior probability of each class : \",file=f)\n",
    "\n",
    "sum = np.sum(y_np)\n",
    "print(sum,file=f)\n",
    "index = 0\n",
    "for i in x_np:\n",
    "    prior = y_np[index]/sum\n",
    "    index = index + 1\n",
    "    print(i + ': ' + str(prior), file=f)\n",
    "\n",
    "# f) the size of the vocabulary(the number of different words)\n",
    "print(\"(f) the size of the vocabulary(the number of different words\",file=f)\n",
    "print(len(vocabulary_corpus),file=f)\n",
    "# g) the number of word-tokens in each class\n",
    "print(\"(g) the number of word-tokens in each class: \",file=f)\n",
    "for class_name in corpus_category_names:\n",
    "    # print(class_name)\n",
    "    class_data = load_files(\"BBC\", categories=class_name, encoding='latin1')\n",
    "    class_data_data = class_data.data\n",
    "    class_vec = CountVectorizer()\n",
    "    class_array = class_vec.fit_transform(class_data_data).toarray()\n",
    "    sum_class = class_array.sum()\n",
    "\n",
    "    \n",
    "    print(\"The number of word-tokens in \" + class_name + \": \" + str(sum_class),file=f)\n",
    "    \n",
    "# h) the number of word-tokens in the entire corpus\n",
    "print('(h) the number of word-tokens in the entire corpus: ',file=f)\n",
    "sum_total = vector.toarray().sum() \n",
    "print(\"Total word-tokens in entire corpus: \" + str(sum_total),file=f)\n",
    "\n",
    "# i) the number and percentage of words with freuency of zero in each class\n",
    "print('(i) the number and percentage of words with freuency of zero in each class: ',file=f)\n",
    "for class_name in corpus_category_names:\n",
    "    # print(class_name)\n",
    "    class_data = load_files(\"BBC\", categories=class_name, encoding='latin1')\n",
    "    class_data_data = class_data.data\n",
    "    class_vec = CountVectorizer()\n",
    "    class_vec.fit(class_data_data)\n",
    "    num_feat_word = len(class_vec.get_feature_names())\n",
    "    num_zero = len(vocabulary_corpus)-num_feat_word\n",
    "    print(\"The number of words with frequency of zero in \" + class_name + \": \"+ str(num_zero),file=f)  # frequency , which divide which\n",
    "    print(\"The percentage of words with frequency of zero in \" + class_name + \": \"+ str(num_zero/len(vocabulary_corpus)),file=f)\n",
    "\n",
    "# j) the number and percentage of words with a frequency of 1 in the entire corpus\n",
    "print('(j) the number and percentage of words with a frequency of 1 in the entire corpus: ',file=f)\n",
    "count_word = 0\n",
    "for word in (vector.toarray().sum(axis = 0)):\n",
    "    if word == 1:\n",
    "        count_word = count_word + 1\n",
    "print(\"The number of words with a frequency of 1 in entire corpus: \" + str(count_word),file=f)\n",
    "print(\"The percentage of words with a frequency of 1 in entire corpus: \" + str(count_word/sum_total),file=f)\n",
    "\n",
    "\n",
    "counts = pd.DataFrame(vector.toarray(), columns=vectorizer.get_feature_names())\n",
    "print(\"(k) 2 favoirite words and their log-prob\",file=f)\n",
    "# counts\n",
    "# show the top 10 most common words\n",
    "# print(counts.T.sort_values(by=0, ascending=False).head(10))\n",
    "import math\n",
    "count_love = counts['love'].sum()\n",
    "print(\"frequency of 'love': \" + str(count_love),file=f)\n",
    "log_prob_love = math.log(count_love/sum_total)\n",
    "print(\"the log-prob of the 'love' favorite word is: \" + str(log_prob_love),file=f)\n",
    "count_key = counts['key'].sum()\n",
    "print(\"frequency of 'key': \" + str(count_key),file=f)\n",
    "log_prob_key = math.log(count_key/sum_total)\n",
    "print(\"the log-prob of the 'key' favorite word is: \" + str(log_prob_key),file=f)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7b4b4feff2f24a0f0a34464dbe537a36fda679851528fb8735cb41fa49dffb2d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
