{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "#sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 Plot distribution of the instances in each class and save the graphic in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "business\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "510"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numOffolds = len(os.listdir('BBC'))\n",
    "print(numOffolds)\n",
    "all_folds = os.listdir('BBC')\n",
    "folder_path = all_folds[0]\n",
    "print(folder_path)\n",
    "\n",
    "len([x for x in os.listdir('BBC/'+folder_path)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comput No.1 folder.\n",
      "No.1 has 510 files.\n",
      "comput No.2 folder.\n",
      "No.2 has 386 files.\n",
      "comput No.3 folder.\n",
      "No.3 has 417 files.\n",
      "comput No.4 folder.\n",
      "No.4 has 511 files.\n",
      "comput No.5 folder.\n",
      "No.5 has 401 files.\n"
     ]
    }
   ],
   "source": [
    "data_need = []\n",
    "data_x = []\n",
    "data_y = []\n",
    "\n",
    "\n",
    "path = 'BBC/'\n",
    "all_folds = os.listdir(path)\n",
    "\n",
    "for i in range (len(all_folds)):\n",
    "    data_x.append(all_folds[i])\n",
    "    print(f'comput No.{i+1} folder.')\n",
    "    folder_path = all_folds[i]\n",
    "    NumOfFiles = len([x for x in os.listdir(path + folder_path)])\n",
    "    data_y.append(NumOfFiles)\n",
    "    print(f'No.{i+1} has {NumOfFiles} files.')\n",
    "    table = pd.DataFrame({'folder_id': all_folds[i],'file_num':[NumOfFiles]})\n",
    "    data_need.append(table)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       folder_id  file_num\n",
      "0       business       510\n",
      "0  entertainment       386\n",
      "0       politics       417\n",
      "0          sport       511\n",
      "0           tech       401\n"
     ]
    }
   ],
   "source": [
    "data_final = pd.concat(data_need, axis=0)\n",
    "print(data_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['business' 'entertainment' 'politics' 'sport' 'tech']\n",
      "[510 386 417 511 401]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_np = np.array(data_x)\n",
    "print(x_np)\n",
    "y_np = np.array(data_y)\n",
    "print(y_np)\n",
    "plt.title(\"Distribution of the instances in each class\")\n",
    "plt.xlabel(\"class name\")\n",
    "plt.ylabel(\"number of instances\")\n",
    "plt.bar(x_np,y_np)\n",
    "plt.savefig('BBC-Distribution.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 4 2 ... 1 1 3]\n",
      "['business', 'entertainment', 'politics', 'sport', 'tech']\n"
     ]
    }
   ],
   "source": [
    "# use load_files to read the file structure and assign the category name to each file\n",
    "corpus_data = load_files(\"BBC\",encoding=\"latin1\")\n",
    "corpus_data_data = corpus_data.data\n",
    "# print(corpus_data_data)\n",
    "corpus_category = corpus_data.target\n",
    "print(corpus_category)\n",
    "corpus_category_names = corpus_data.target_names\n",
    "print(corpus_category_names)\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 4 2 ... 1 1 3]\n",
      "['business', 'entertainment', 'politics', 'sport', 'tech']\n",
      "['BBC\\\\business\\\\385.txt' 'BBC\\\\tech\\\\160.txt' 'BBC\\\\politics\\\\090.txt'\n",
      " ... 'BBC\\\\entertainment\\\\254.txt' 'BBC\\\\entertainment\\\\326.txt'\n",
      " 'BBC\\\\sport\\\\341.txt']\n"
     ]
    }
   ],
   "source": [
    "print (corpus_category)\n",
    "print(corpus_category_names)\n",
    "print(corpus_data.filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 Pre-process the dataset to have the features ready to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: \n",
      "{'one': 12, 'of': 11, 'the': 15, 'most': 9, 'basic': 1, 'ways': 18, 'we': 19, 'can': 3, 'numerically': 10, 'represent': 13, 'words': 20, 'is': 7, 'through': 16, 'hot': 6, 'encoding': 5, 'method': 8, 'also': 0, 'sometimes': 14, 'called': 2, 'count': 4, 'vectorizing': 17}\n"
     ]
    }
   ],
   "source": [
    "sample_text = [\"One of the most basic ways we can numerically represent words \"\n",
    "               \"is through the one-hot encoding method (also sometimes called \"\n",
    "               \"count vectorizing).\"]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(sample_text)\n",
    "print('Vocabulary: ')\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 12)\t2\n",
      "  (0, 13)\t1\n",
      "  (0, 14)\t1\n",
      "  (0, 15)\t2\n",
      "  (0, 16)\t1\n",
      "  (0, 17)\t1\n",
      "  (0, 18)\t1\n",
      "  (0, 19)\t1\n",
      "  (0, 20)\t1\n"
     ]
    }
   ],
   "source": [
    "vector = vectorizer.transform(sample_text)\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 2 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29421\n",
      "(2225, 29421)\n",
      "836357\n"
     ]
    }
   ],
   "source": [
    "#create a Vectorizer Object\n",
    "vectorizer = CountVectorizer()\n",
    "#tokenize and build vocab\n",
    "vectorizer.fit(corpus_data_data)\n",
    "#summarize\n",
    "vocabulary_corpus = vectorizer.get_feature_names()\n",
    "#print(vocabulary_corpus)\n",
    "print(len(vectorizer.get_feature_names()))\n",
    "\n",
    "\n",
    "# Encode the dataset to document-term matrix\n",
    "vector = vectorizer.transform(corpus_data_data)\n",
    "print(vector.shape)\n",
    "print(vector.toarray().sum())#this sum for what to use?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hot vector: \n",
      "[[0 0 0 ... 0 0 0]]\n",
      "Hot and one: \n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "[[1 1 1 1 2 1 3]]\n"
     ]
    }
   ],
   "source": [
    "print('Hot vector: ')\n",
    "print(vectorizer.transform(['hot']).toarray())\n",
    "print('Hot and one: ')\n",
    "print(vectorizer.transform(['hot', 'one']).toarray())\n",
    "new_text = ['Today is the day that I do the thing today, today']\n",
    "new_vectorizer = CountVectorizer()\n",
    "print(new_vectorizer.fit_transform(new_text).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bird', 'cat', 'dog', 'fish']\n",
      "[[0 1 1 1]\n",
      " [0 2 1 0]\n",
      " [1 0 0 1]\n",
      " [1 0 0 0]]\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "texts=[\"dog cat fish\",\"dog cat cat\",\"fish bird\", 'bird']\n",
    "cv = CountVectorizer()\n",
    "cv_fit=cv.fit_transform(texts)\n",
    "\n",
    "print(cv.get_feature_names())\n",
    "print(cv_fit.toarray())\n",
    "print(cv_fit.toarray().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 Split the dataset int 80% for training 20% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into 80% for training and 20% for testing\n",
    "X = corpus_data_data\n",
    "y = corpus_category\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, train_size=0.8, random_state= None)\n",
    "#print(X_train)\n",
    "#print(y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26992\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0001</th>\n",
       "      <th>000bn</th>\n",
       "      <th>000m</th>\n",
       "      <th>000th</th>\n",
       "      <th>001</th>\n",
       "      <th>001and</th>\n",
       "      <th>001st</th>\n",
       "      <th>004</th>\n",
       "      <th>...</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zooms</th>\n",
       "      <th>zooropa</th>\n",
       "      <th>zornotza</th>\n",
       "      <th>zorro</th>\n",
       "      <th>zuluaga</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zutons</th>\n",
       "      <th>zvonareva</th>\n",
       "      <th>zvyagintsev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1775</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1776</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1777</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1778</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1779</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1780 rows × 26992 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     00 000 0001 000bn 000m 000th 001 001and 001st 004  ... zoom zooms  \\\n",
       "0     0   0    0     0    0     0   0      0     0   0  ...    0     0   \n",
       "1     0   0    0     0    0     0   0      0     0   0  ...    0     0   \n",
       "2     0   0    0     0    0     0   0      0     0   0  ...    0     0   \n",
       "3     0   0    0     0    0     0   0      0     0   0  ...    0     0   \n",
       "4     0   0    0     0    0     0   0      0     0   0  ...    0     0   \n",
       "...  ..  ..  ...   ...  ...   ...  ..    ...   ...  ..  ...  ...   ...   \n",
       "1775  0   0    0     0    0     0   0      0     0   0  ...    0     0   \n",
       "1776  0   3    0     0    0     0   0      0     0   0  ...    0     0   \n",
       "1777  0   0    0     0    0     0   0      0     0   0  ...    0     0   \n",
       "1778  0   0    1     0    0     0   0      0     0   0  ...    0     0   \n",
       "1779  0   0    0     0    0     0   0      0     0   0  ...    0     0   \n",
       "\n",
       "     zooropa zornotza zorro zuluaga zurich zutons zvonareva zvyagintsev  \n",
       "0          0        0     0       0      0      0         0           0  \n",
       "1          0        0     0       0      0      0         0           0  \n",
       "2          0        0     0       0      0      0         0           0  \n",
       "3          0        0     0       0      0      0         0           0  \n",
       "4          0        0     0       0      0      0         0           0  \n",
       "...      ...      ...   ...     ...    ...    ...       ...         ...  \n",
       "1775       0        0     0       0      0      0         0           0  \n",
       "1776       0        0     0       0      0      0         0           0  \n",
       "1777       0        0     0       0      0      0         0           0  \n",
       "1778       0        0     0       0      0      0         0           0  \n",
       "1779       0        0     0       0      0      0         0           0  \n",
       "\n",
       "[1780 rows x 26992 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# countVectorizer\n",
    "vec = CountVectorizer()\n",
    "# fit the vectorizer on the training dataset\n",
    "vec.fit(X_train)\n",
    "print(len(vec.get_feature_names()))\n",
    "vocab = vec.get_feature_names()\n",
    "# print(vocab)\n",
    "X_train_transformed = vec.transform(X_train)\n",
    "# print(X_train_transformed)\n",
    "# convert training set to document-term matrix\n",
    "X_train_feat = X_train_transformed.toarray()\n",
    "# convert to sparse matrix for readability\n",
    "pd.DataFrame(X_train_feat,columns= [vec.get_feature_names()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0001</th>\n",
       "      <th>000bn</th>\n",
       "      <th>000m</th>\n",
       "      <th>000th</th>\n",
       "      <th>001</th>\n",
       "      <th>001and</th>\n",
       "      <th>001st</th>\n",
       "      <th>004</th>\n",
       "      <th>...</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zooms</th>\n",
       "      <th>zooropa</th>\n",
       "      <th>zornotza</th>\n",
       "      <th>zorro</th>\n",
       "      <th>zuluaga</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zutons</th>\n",
       "      <th>zvonareva</th>\n",
       "      <th>zvyagintsev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>445 rows × 26992 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    00 000 0001 000bn 000m 000th 001 001and 001st 004  ... zoom zooms zooropa  \\\n",
       "0    0   3    0     0    0     0   0      0     0   0  ...    0     0       0   \n",
       "1    0   0    0     0    0     0   0      0     0   0  ...    0     0       0   \n",
       "2    0   0    0     0    0     0   0      0     0   0  ...    0     0       0   \n",
       "3    0   1    0     0    0     0   0      0     0   0  ...    0     0       0   \n",
       "4    0   0    0     0    0     0   0      0     0   0  ...    0     0       0   \n",
       "..  ..  ..  ...   ...  ...   ...  ..    ...   ...  ..  ...  ...   ...     ...   \n",
       "440  0   0    0     0    0     0   0      0     0   0  ...    0     0       0   \n",
       "441  0   0    0     0    0     0   0      0     0   0  ...    0     0       0   \n",
       "442  0   0    0     0    0     0   0      0     0   0  ...    0     0       0   \n",
       "443  0   0    0     0    0     0   0      0     0   0  ...    0     0       0   \n",
       "444  0   0    0     0    0     0   0      0     0   0  ...    0     0       0   \n",
       "\n",
       "    zornotza zorro zuluaga zurich zutons zvonareva zvyagintsev  \n",
       "0          0     0       0      0      0         0           0  \n",
       "1          0     0       0      0      0         0           0  \n",
       "2          0     0       0      0      0         0           0  \n",
       "3          0     0       0      0      0         0           0  \n",
       "4          0     0       0      0      0         0           0  \n",
       "..       ...   ...     ...    ...    ...       ...         ...  \n",
       "440        0     0       0      0      0         0           0  \n",
       "441        0     0       0      0      0         0           0  \n",
       "442        0     0       0      0      0         0           0  \n",
       "443        0     0       0      0      0         0           0  \n",
       "444        0     0       0      0      0         0           0  \n",
       "\n",
       "[445 rows x 26992 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for test data\n",
    "X_test_transformed = vec.transform(X_test)\n",
    "# X_test_transformed\n",
    "# print(X_test_transformed)\n",
    "# convert testing set to document-term matrix\n",
    "X_test_feat = X_test_transformed.toarray()\n",
    "# convert to sparse matrix for readability\n",
    "pd.DataFrame(X_test_feat, columns= [vec.get_feature_names()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 train a multinomial Naive Bayes Classifier on the training set using the default parameters and evaluate it on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------MultinomialNB default values, try 1-----------------------------------\n",
      "(b) the confusion matrix: \n",
      "[[93  0  0  0  4]\n",
      " [ 1 79  2  0  1]\n",
      " [ 0  0 89  0  0]\n",
      " [ 0  0  0 90  0]\n",
      " [ 0  0  3  0 83]]\n",
      "(c) the precision, recall and F1-measure for each class of the test set : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.97        97\n",
      "           1       1.00      0.95      0.98        83\n",
      "           2       0.95      1.00      0.97        89\n",
      "           3       1.00      1.00      1.00        90\n",
      "           4       0.94      0.97      0.95        86\n",
      "\n",
      "    accuracy                           0.98       445\n",
      "   macro avg       0.98      0.98      0.98       445\n",
      "weighted avg       0.98      0.98      0.98       445\n",
      "\n",
      "(d) the accuracy, macro-average F1 and weighted-average F1 of the model: \n",
      "Accuracy score of the test set is : 0.9752808988764045\n",
      "Macro average F1 of the test set is : 0.9751662431276534\n",
      "Weighted average F1 of the test set is : 0.9753385017767798\n"
     ]
    }
   ],
   "source": [
    "# multinomial Naive Bayes\n",
    "print(\"--------------------------------MultinomialNB default values, try 1-----------------------------------\")\n",
    "nb = MultinomialNB(alpha=1)\n",
    "# fit the training set using the default parameters and evaluate it on the test set\n",
    "nb.fit(X_train_transformed, y_train)\n",
    "\n",
    "# predict, Perform classification on an array of test vectors X.\n",
    "y_pred_class = nb.predict(X_test_transformed)\n",
    "# predict probabilities, Return probability estimates for the test vector X.\n",
    "y_pred_proba = nb.predict_proba(X_test_transformed)\n",
    "\n",
    "# b) confusion matrix\n",
    "from sklearn import metrics\n",
    "confusion = metrics.confusion_matrix(y_test, y_pred_class)\n",
    "print(\"(b) the confusion matrix: \")\n",
    "print(confusion)\n",
    "\n",
    "\n",
    "\n",
    "# c) the precision, recall and F1-measure for each class\n",
    "print(\"(c) the precision, recall and F1-measure for each class of the test set : \")\n",
    "print(metrics.classification_report(y_test, y_pred_class))\n",
    "\n",
    "\n",
    "\n",
    "# d) the accuracy, macro-average F1 and weighted-average F1 of the model\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred_class)\n",
    "print(\"(d) the accuracy, macro-average F1 and weighted-average F1 of the model: \")\n",
    "print(\"Accuracy score of the test set is : \" + str(accuracy))\n",
    "\n",
    "macro_avg_F1 = metrics.f1_score(y_test, y_pred_class, average='macro')\n",
    "print(\"Macro average F1 of the test set is : \" + str(macro_avg_F1))\n",
    "weighted_avg_F1 = metrics.f1_score(y_test, y_pred_class, average='weighted')\n",
    "print(\"Weighted average F1 of the test set is : \" + str(weighted_avg_F1))\n",
    "\n",
    "# index = nb.predict(vec.transform(['Need to restart economy but with caution: Yogi Adityanath at E-Agenda AajTak']))\n",
    "\n",
    "# def type_check(i):\n",
    "#     if i == 0:\n",
    "#         print('business')\n",
    "#     elif i == 1:\n",
    "#         print('entertainment')\n",
    "#     elif i == 2:\n",
    "#         print('politics')\n",
    "#     elif i == 3:\n",
    "#         print('sport')\n",
    "#     elif i == 4:\n",
    "#         print('tech')\n",
    "# type_check(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(e) the prior probability of each class : \n",
      "2225\n",
      "business: 0.2292134831460674\n",
      "entertainment: 0.17348314606741572\n",
      "politics: 0.18741573033707865\n",
      "sport: 0.22966292134831462\n",
      "tech: 0.1802247191011236\n"
     ]
    }
   ],
   "source": [
    "# e) the prior probability of each class\n",
    "print(\"(e) the prior probability of each class : \")\n",
    "\n",
    "sum = np.sum(y_np)\n",
    "print(sum)\n",
    "index = 0\n",
    "for i in x_np:\n",
    "    prior = y_np[index]/sum\n",
    "    index = index + 1\n",
    "    print(i + ': ' + str(prior))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(f) the size of the vocabulary(the number of different words\n",
      "29421\n"
     ]
    }
   ],
   "source": [
    "# f) the size of the vocabulary(the number of different words)\n",
    "print(\"(f) the size of the vocabulary(the number of different words\")\n",
    "print(len(vocabulary_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(g) the number of word-tokens in each class: \n",
      "The number of word-tokens in business: 164663\n",
      "The number of word-tokens in entertainment: 124893\n",
      "The number of word-tokens in politics: 185208\n",
      "The number of word-tokens in sport: 162953\n",
      "The number of word-tokens in tech: 198640\n"
     ]
    }
   ],
   "source": [
    "# g) the number of word-tokens in each class\n",
    "print(\"(g) the number of word-tokens in each class: \")\n",
    "for class_name in corpus_category_names:\n",
    "    # print(class_name)\n",
    "    class_data = load_files(\"BBC\", categories=class_name, encoding='latin1')\n",
    "    class_data_data = class_data.data\n",
    "    class_vec = CountVectorizer()\n",
    "    class_array = class_vec.fit_transform(class_data_data).toarray()\n",
    "    sum_class = class_array.sum()\n",
    "\n",
    "    \n",
    "    print(\"The number of word-tokens in \" + class_name + \": \" + str(sum_class))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(h) the number of word-tokens in the entire corpus: \n",
      "Total word-tokens in entire corpus: 836357\n",
      "(j) the number and percentage of words with a frequency of 1 in the entire corpus: \n",
      "The number of words with a frequency of 1 in entire corpus: 10005\n",
      "The percentage of words with a frequency of 1 in entire corpus: 0.01196259492059013\n"
     ]
    }
   ],
   "source": [
    "# h) the number of word-tokens in the entire corpus\n",
    "print('(h) the number of word-tokens in the entire corpus: ')\n",
    "sum_total = vector.toarray().sum() \n",
    "print(\"Total word-tokens in entire corpus: \" + str(sum_total))\n",
    "\n",
    "# j) the number and percentage of words with a frequency of 1 in the entire corpus\n",
    "print('(j) the number and percentage of words with a frequency of 1 in the entire corpus: ')\n",
    "count_word = 0\n",
    "for word in (vector.toarray().sum(axis = 0)):\n",
    "    if word == 1:\n",
    "        count_word = count_word + 1\n",
    "print(\"The number of words with a frequency of 1 in entire corpus: \" + str(count_word))\n",
    "print(\"The percentage of words with a frequency of 1 in entire corpus: \" + str(count_word/sum_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(i) the number and percentage of words with freuency of zero in each class: \n",
      "The number of words with frequency of zero in business: 17538\n",
      "The percentage of words with frequency of zero in business: 0.5961048230855511\n",
      "The number of words with frequency of zero in entertainment: 17746\n",
      "The percentage of words with frequency of zero in entertainment: 0.6031746031746031\n",
      "The number of words with frequency of zero in politics: 18201\n",
      "The percentage of words with frequency of zero in politics: 0.6186397471194045\n",
      "The number of words with frequency of zero in sport: 18850\n",
      "The percentage of words with frequency of zero in sport: 0.6406988205703409\n",
      "The number of words with frequency of zero in tech: 17323\n",
      "The percentage of words with frequency of zero in tech: 0.5887971177050406\n"
     ]
    }
   ],
   "source": [
    "# i) the number and percentage of words with freuency of zero in each class\n",
    "print('(i) the number and percentage of words with freuency of zero in each class: ')\n",
    "for class_name in corpus_category_names:\n",
    "    # print(class_name)\n",
    "    class_data = load_files(\"BBC\", categories=class_name, encoding='latin1')\n",
    "    class_data_data = class_data.data\n",
    "    class_vec = CountVectorizer()\n",
    "    class_vec.fit(class_data_data)\n",
    "    num_feat_word = len(class_vec.get_feature_names())\n",
    "    num_zero = len(vocabulary_corpus)-num_feat_word\n",
    "    print(\"The number of words with frequency of zero in \" + class_name + \": \"+ str(num_zero))\n",
    "    print(\"The percentage of words with frequency of zero in \" + class_name + \": \"+ str(num_zero/len(vocabulary_corpus)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(k) 2 favoirite words and their log-prob\n",
      "frequency of 'love': 151\n",
      "the log-prob of the 'love' favorite word is: -8.619530997591884\n",
      "frequency of 'key': 245\n",
      "the log-prob of the 'key' favorite word is: -8.135552623862083\n"
     ]
    }
   ],
   "source": [
    "\n",
    "counts = pd.DataFrame(vector.toarray(), columns=vectorizer.get_feature_names())\n",
    "print(\"(k) 2 favoirite words and their log-prob\")\n",
    "# counts\n",
    "# show the top 10 most common words\n",
    "# print(counts.T.sort_values(by=0, ascending=False).head(10))\n",
    "import math\n",
    "count_love = counts['love'].sum()\n",
    "print(\"frequency of 'love': \" + str(count_love))\n",
    "log_prob_love = math.log(count_love/sum_total)\n",
    "print(\"the log-prob of the 'love' favorite word is: \" + str(log_prob_love))\n",
    "count_key = counts['key'].sum()\n",
    "print(\"frequency of 'key': \" + str(count_key))\n",
    "log_prob_key = math.log(count_key/sum_total)\n",
    "print(\"the log-prob of the 'key' favorite word is: \" + str(log_prob_key))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Redo steps 6 and 7 without changing anything (do not redo step 5, the dataset split). Change the model name to something like \\MultinomialNB default values, try 2\" and append the results to the  tle  bbc-performance.txt.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------MultinomialNB default values, try 2-----------------------------------\n",
      "(b) the confusion matrix: \n",
      "[[ 77   0   3   0   3]\n",
      " [  0  77   1   0   2]\n",
      " [  1   0  88   0   0]\n",
      " [  0   0   0 112   0]\n",
      " [  0   1   0   0  80]]\n",
      "(c) the precision, recall and F1-measure for each class of the test set : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.93      0.96        83\n",
      "           1       0.99      0.96      0.97        80\n",
      "           2       0.96      0.99      0.97        89\n",
      "           3       1.00      1.00      1.00       112\n",
      "           4       0.94      0.99      0.96        81\n",
      "\n",
      "    accuracy                           0.98       445\n",
      "   macro avg       0.97      0.97      0.97       445\n",
      "weighted avg       0.98      0.98      0.98       445\n",
      "\n",
      "(d) the accuracy, macro-average F1 and weighted-average F1 of the model: \n",
      "Accuracy score of the test set is : 0.9752808988764045\n",
      "Macro average F1 of the test set is : 0.9734872791457427\n",
      "Weighted average F1 of the test set is : 0.9752353112648198\n"
     ]
    }
   ],
   "source": [
    "# multinomial Naive Bayes\n",
    "print(\"--------------------------------MultinomialNB default values, try 2-----------------------------------\")\n",
    "nb = MultinomialNB(alpha=2)\n",
    "# fit the training set using the default parameters and evaluate it on the test set\n",
    "nb.fit(X_train_transformed, y_train)\n",
    "\n",
    "# predict, Perform classification on an array of test vectors X.\n",
    "y_pred_class = nb.predict(X_test_transformed)\n",
    "# predict probabilities, Return probability estimates for the test vector X.\n",
    "y_pred_proba = nb.predict_proba(X_test_transformed)\n",
    "\n",
    "# b) confusion matrix\n",
    "from sklearn import metrics\n",
    "confusion = metrics.confusion_matrix(y_test, y_pred_class)\n",
    "print(\"(b) the confusion matrix: \")\n",
    "print(confusion)\n",
    "\n",
    "\n",
    "\n",
    "# c) the precision, recall and F1-measure for each class\n",
    "print(\"(c) the precision, recall and F1-measure for each class of the test set : \")\n",
    "print(metrics.classification_report(y_test, y_pred_class))\n",
    "\n",
    "\n",
    "\n",
    "# d) the accuracy, macro-average F1 and weighted-average F1 of the model\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred_class)\n",
    "print(\"(d) the accuracy, macro-average F1 and weighted-average F1 of the model: \")\n",
    "print(\"Accuracy score of the test set is : \" + str(accuracy))\n",
    "\n",
    "macro_avg_F1 = metrics.f1_score(y_test, y_pred_class, average='macro')\n",
    "print(\"Macro average F1 of the test set is : \" + str(macro_avg_F1))\n",
    "weighted_avg_F1 = metrics.f1_score(y_test, y_pred_class, average='weighted')\n",
    "print(\"Weighted average F1 of the test set is : \" + str(weighted_avg_F1))\n",
    "\n",
    "# index = nb.predict(vec.transform(['Need to restart economy but with caution: Yogi Adityanath at E-Agenda AajTak']))\n",
    "\n",
    "# def type_check(i):\n",
    "#     if i == 0:\n",
    "#         print('business')\n",
    "#     elif i == 1:\n",
    "#         print('entertainment')\n",
    "#     elif i == 2:\n",
    "#         print('politics')\n",
    "#     elif i == 3:\n",
    "#         print('sport')\n",
    "#     elif i == 4:\n",
    "#         print('tech')\n",
    "# type_check(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(e)the prior probility of each class :\n",
      "2225\n",
      "business:0.2292134831460674\n",
      "entertainment:0.17348314606741572\n",
      "politics:0.18741573033707865\n",
      "sport:0.22966292134831462\n",
      "tech:0.1802247191011236\n"
     ]
    }
   ],
   "source": [
    "#E) The prior probability of each class\n",
    "print(\"(e)the prior probility of each class :\")\n",
    "\n",
    "sum=np.sum(y_np)\n",
    "print(sum)\n",
    "#print(y_np[0])\n",
    "index=0\n",
    "for i in x_np:\n",
    "    prior=y_np[index]/sum\n",
    "    index=index+1\n",
    "    print(i+':'+str(prior))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(f) the size of the vocabulary(the number of different words)\n",
      "29421\n"
     ]
    }
   ],
   "source": [
    "#f) the size of the vocabulary(the number of  different words)\n",
    "v=len(vocabulary_corpus)\n",
    "print(\"(f) the size of the vocabulary(the number of different words)\")\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(g)the number of word-tokens in each class:\n",
      "The number of word-tokens in  business:223505\n",
      "The number of word-tokens in  entertainment:183735\n",
      "The number of word-tokens in  politics:244050\n",
      "The number of word-tokens in  sport:221795\n",
      "The number of word-tokens in  tech:257482\n"
     ]
    }
   ],
   "source": [
    "#g)the number of word-tokens in each class\n",
    "print(\"(g)the number of word-tokens in each class:\")\n",
    "for class_name in corpus_category_names:\n",
    "    #print(class name)\n",
    "    class_data=load_files(\"BBC\",categories=class_name,encoding='latin1')\n",
    "    class_data_data=class_data.data\n",
    "    class_vec=CountVectorizer()\n",
    "    class_array=class_vec.fit_transform(class_data_data).toarray()\n",
    "    sum_class=class_array.sum()+2*v\n",
    "    \n",
    "    print(\"The number of word-tokens in  \"+class_name+\":\"+str(sum_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(h)the number of word-tokens in the entire corpus:\n",
      "Total word-tokens in entire corpus:895199\n",
      "(j) the number and percentage of words with a frequency of 1 in the entire corpus:\n",
      "The number of words with a frequency of 1 in entire corpus: 0\n",
      "The percentage of words with a frequency of 1 in entire corpus :0.0\n"
     ]
    }
   ],
   "source": [
    "#h)the number of word-token in the entir corpus\n",
    "print(\"(h)the number of word-tokens in the entire corpus:\")\n",
    "sum_total=vector.toarray().sum()+2*v\n",
    "\n",
    "print(\"Total word-tokens in entire corpus:\"+str(sum_total))# add 2*v\n",
    "\n",
    "#j)the number and percentage of words with a frequecny of 1 in the entire corpus\n",
    "print('(j) the number and percentage of words with a frequency of 1 in the entire corpus:')\n",
    "cout_word=0\n",
    "#for word in (vector.toarray().sum(axis=0)):#axis=0 sum the column number\n",
    "    #if word==1:\n",
    "        #count_word=count_word+1\n",
    "        \n",
    "print(\"The number of words with a frequency of 1 in entire corpus: \"+ str(cout_word))\n",
    "p=cout_word/sum_total\n",
    "\n",
    "print(\"The percentage of words with a frequency of 1 in entire corpus :\"+str(p))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(i)the number and percentage of words with freuency of zero in each class:\n",
      "The number of words with frequency of zero in business:0\n",
      "The percentage of words with frequency of zero in business:0.0\n",
      "The number of words with frequency of zero in entertainment:0\n",
      "The percentage of words with frequency of zero in entertainment:0.0\n",
      "The number of words with frequency of zero in politics:0\n",
      "The percentage of words with frequency of zero in politics:0.0\n",
      "The number of words with frequency of zero in sport:0\n",
      "The percentage of words with frequency of zero in sport:0.0\n",
      "The number of words with frequency of zero in tech:0\n",
      "The percentage of words with frequency of zero in tech:0.0\n"
     ]
    }
   ],
   "source": [
    "# i)the number and percentage of words with frequency of zero in each class\n",
    "print(\"(i)the number and percentage of words with freuency of zero in each class:\")\n",
    "for class_name in corpus_category_names:\n",
    "    #print(class_name)\n",
    "    class_data=load_files(\"BBC\",categories=class_name,encoding='latin1')\n",
    "    class_data_data=class_data.data\n",
    "    class_vec=CountVectorizer()\n",
    "    class_vec.fit(class_data.data)\n",
    "    num_feat_word=len(class_vec.get_feature_names())\n",
    "    num_zero=len(vocabulary_corpus)-num_feat_word ###why?\n",
    "    print(\"The number of words with frequency of zero in \"+class_name+\":\"+str(num_zero-num_zero))\n",
    "    print(\"The percentage of words with frequency of zero in \"+class_name+\":\"+str((num_zero-num_zero)/len(vocabulary_corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(k) 2 favorite words and their log-prob\n",
      "frequency of 'love':153\n",
      "The log-prob of the 'love' favorite word is :-8.73802400505371\n",
      "frequency of 'key':247\n",
      "The log-prob of the 'key'favorite word is :-8.04209945480497\n"
     ]
    }
   ],
   "source": [
    "# k)Two favorite words and theri log-prob\n",
    "counts=pd.DataFrame(vector.toarray(),columns=vectorizer.get_feature_names())\n",
    "print(\"(k) 2 favorite words and their log-prob\")\n",
    "#counts\n",
    "#show the top 10 most common words\n",
    "#print(counts.T.sort_values(by=0,ascending=False).head(10))\n",
    "import math\n",
    "count_love=counts['love'].sum()\n",
    "count_love_smooth=count_love+2\n",
    "print(\"frequency of 'love':\"+ str(count_love_smooth))\n",
    "#print(sum_total)\n",
    "sum_total_smooth=sum_total+2*v\n",
    "#print(sum_total_smooth)\n",
    "log_prob_love=math.log(count_love_smooth/sum_total_smooth)\n",
    "print(\"The log-prob of the 'love' favorite word is :\"+ str(log_prob_love))\n",
    "count_key=counts['key'].sum()\n",
    "count_key_smooth=count_key+2\n",
    "print(\"frequency of 'key':\"+str(count_key_smooth))\n",
    "log_prob_key=math.log(count_key_smooth/sum_total_smooth)\n",
    "print(\"The log-prob of the 'key'favorite word is :\"+str(log_prob_free))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Redo steps 6 and 7 again, but this time, change the smoothing value to 0.0001. Append the results at the end of bbc-performance.txt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------MultinomialNB default values, try 0.0001-----------------------------------\n",
      "(b) the confusion matrix: \n",
      "[[ 75   0   3   0   5]\n",
      " [  0  77   2   0   1]\n",
      " [  3   0  86   0   0]\n",
      " [  0   0   0 112   0]\n",
      " [  2   2   0   0  77]]\n",
      "(c) the precision, recall and F1-measure for each class of the test set : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.90      0.92        83\n",
      "           1       0.97      0.96      0.97        80\n",
      "           2       0.95      0.97      0.96        89\n",
      "           3       1.00      1.00      1.00       112\n",
      "           4       0.93      0.95      0.94        81\n",
      "\n",
      "    accuracy                           0.96       445\n",
      "   macro avg       0.96      0.96      0.96       445\n",
      "weighted avg       0.96      0.96      0.96       445\n",
      "\n",
      "(d) the accuracy, macro-average F1 and weighted-average F1 of the model: \n",
      "Accuracy score of the test set is : 0.9595505617977528\n",
      "Macro average F1 of the test set is : 0.9566757607383922\n",
      "Weighted average F1 of the test set is : 0.9594832918694827\n"
     ]
    }
   ],
   "source": [
    "# multinomial Naive Bayes\n",
    "print(\"--------------------------------MultinomialNB default values, try 0.0001-----------------------------------\")\n",
    "nb = MultinomialNB(alpha=0.0001)\n",
    "# fit the training set using the default parameters and evaluate it on the test set\n",
    "nb.fit(X_train_transformed, y_train)\n",
    "\n",
    "# predict, Perform classification on an array of test vectors X.\n",
    "y_pred_class = nb.predict(X_test_transformed)\n",
    "# predict probabilities, Return probability estimates for the test vector X.\n",
    "y_pred_proba = nb.predict_proba(X_test_transformed)\n",
    "\n",
    "# b) confusion matrix\n",
    "from sklearn import metrics\n",
    "confusion = metrics.confusion_matrix(y_test, y_pred_class)\n",
    "print(\"(b) the confusion matrix: \")\n",
    "print(confusion)\n",
    "\n",
    "\n",
    "\n",
    "# c) the precision, recall and F1-measure for each class\n",
    "print(\"(c) the precision, recall and F1-measure for each class of the test set : \")\n",
    "print(metrics.classification_report(y_test, y_pred_class))\n",
    "\n",
    "\n",
    "\n",
    "# d) the accuracy, macro-average F1 and weighted-average F1 of the model\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred_class)\n",
    "print(\"(d) the accuracy, macro-average F1 and weighted-average F1 of the model: \")\n",
    "print(\"Accuracy score of the test set is : \" + str(accuracy))\n",
    "\n",
    "macro_avg_F1 = metrics.f1_score(y_test, y_pred_class, average='macro')\n",
    "print(\"Macro average F1 of the test set is : \" + str(macro_avg_F1))\n",
    "weighted_avg_F1 = metrics.f1_score(y_test, y_pred_class, average='weighted')\n",
    "print(\"Weighted average F1 of the test set is : \" + str(weighted_avg_F1))\n",
    "\n",
    "# index = nb.predict(vec.transform(['Need to restart economy but with caution: Yogi Adityanath at E-Agenda AajTak']))\n",
    "\n",
    "# def type_check(i):\n",
    "#     if i == 0:\n",
    "#         print('business')\n",
    "#     elif i == 1:\n",
    "#         print('entertainment')\n",
    "#     elif i == 2:\n",
    "#         print('politics')\n",
    "#     elif i == 3:\n",
    "#         print('sport')\n",
    "#     elif i == 4:\n",
    "#         print('tech')\n",
    "# type_check(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(e)the prior probility of each class :\n",
      "2225\n",
      "business:0.2292134831460674\n",
      "entertainment:0.17348314606741572\n",
      "politics:0.18741573033707865\n",
      "sport:0.22966292134831462\n",
      "tech:0.1802247191011236\n"
     ]
    }
   ],
   "source": [
    "#E) The prior probability of each class\n",
    "print(\"(e)the prior probility of each class :\")\n",
    "\n",
    "sum=np.sum(y_np)\n",
    "print(sum)\n",
    "#print(y_np[0])\n",
    "index=0\n",
    "for i in x_np:\n",
    "    prior=y_np[index]/sum\n",
    "    index=index+1\n",
    "    print(i+':'+str(prior))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(f) the size of the vocabulary(the number of different words)\n",
      "29421\n"
     ]
    }
   ],
   "source": [
    "#f) the size of the vocabulary(the number of  different words)\n",
    "v=len(vocabulary_corpus)\n",
    "print(\"(f) the size of the vocabulary(the number of different words)\")\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(g)the number of word-tokens in each class:\n",
      "The number of word-tokens in  business:164665.9421\n",
      "The number of word-tokens in  entertainment:124895.9421\n",
      "The number of word-tokens in  politics:185210.9421\n",
      "The number of word-tokens in  sport:162955.9421\n",
      "The number of word-tokens in  tech:198642.9421\n"
     ]
    }
   ],
   "source": [
    "#g)the number of word-tokens in each class\n",
    "print(\"(g)the number of word-tokens in each class:\")\n",
    "for class_name in corpus_category_names:\n",
    "    #print(class name)\n",
    "    class_data=load_files(\"BBC\",categories=class_name,encoding='latin1')\n",
    "    class_data_data=class_data.data\n",
    "    class_vec=CountVectorizer()\n",
    "    class_array=class_vec.fit_transform(class_data_data).toarray()\n",
    "    sum_class=class_array.sum()+0.0001*v\n",
    "    \n",
    "    print(\"The number of word-tokens in  \"+class_name+\":\"+str(sum_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(h)the number of word-tokens in the entire corpus:\n",
      "Total word-tokens in entire corpus:836359.9421\n",
      "(j) the number and percentage of words with a frequency of 1 in the entire corpus:\n",
      "The number of words with a frequency of 1 in entire corpus: 0\n",
      "The percentage of words with a frequency of 1 in entire corpus :0.0\n"
     ]
    }
   ],
   "source": [
    "#h)the number of word-token in the entir corpus\n",
    "print(\"(h)the number of word-tokens in the entire corpus:\")\n",
    "sum_total=vector.toarray().sum()+0.0001*v\n",
    "\n",
    "print(\"Total word-tokens in entire corpus:\"+str(sum_total))# add 2*v\n",
    "\n",
    "#j)the number and percentage of words with a frequecny of 1 in the entire corpus\n",
    "print('(j) the number and percentage of words with a frequency of 1 in the entire corpus:')\n",
    "cout_word=0\n",
    "#for word in (vector.toarray().sum(axis=0)):#axis=0 sum the column number\n",
    "    #if word==1:\n",
    "        #count_word=count_word+1\n",
    "        \n",
    "print(\"The number of words with a frequency of 1 in entire corpus: \"+ str(cout_word))\n",
    "p=cout_word/sum_total\n",
    "\n",
    "print(\"The percentage of words with a frequency of 1 in entire corpus :\"+str(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(i)the number and percentage of words with freuency of zero in each class:\n",
      "The number of words with frequency of zero in business:0\n",
      "The percentage of words with frequency of zero in business:0.0\n",
      "The number of words with frequency of zero in entertainment:0\n",
      "The percentage of words with frequency of zero in entertainment:0.0\n",
      "The number of words with frequency of zero in politics:0\n",
      "The percentage of words with frequency of zero in politics:0.0\n",
      "The number of words with frequency of zero in sport:0\n",
      "The percentage of words with frequency of zero in sport:0.0\n",
      "The number of words with frequency of zero in tech:0\n",
      "The percentage of words with frequency of zero in tech:0.0\n"
     ]
    }
   ],
   "source": [
    "# i)the number and percentage of words with frequency of zero in each class\n",
    "print(\"(i)the number and percentage of words with freuency of zero in each class:\")\n",
    "for class_name in corpus_category_names:\n",
    "    #print(class_name)\n",
    "    class_data=load_files(\"BBC\",categories=class_name,encoding='latin1')\n",
    "    class_data_data=class_data.data\n",
    "    class_vec=CountVectorizer()\n",
    "    class_vec.fit(class_data.data)\n",
    "    num_feat_word=len(class_vec.get_feature_names())\n",
    "    num_zero=len(vocabulary_corpus)-num_feat_word ###why?\n",
    "    print(\"The number of words with frequency of zero in \"+class_name+\":\"+str(num_zero-num_zero))\n",
    "    print(\"The percentage of words with frequency of zero in \"+class_name+\":\"+str((num_zero-num_zero)/len(vocabulary_corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(k) 2 favorite words and their log-prob\n",
      "frequency of 'love':151.0001\n",
      "The log-prob of the 'love' favorite word is :-8.619537370828047\n",
      "frequency of 'key':245.0001\n",
      "The log-prob of the 'key'favorite word is :-8.04209945480497\n"
     ]
    }
   ],
   "source": [
    "# k)Two favorite words and theri log-prob\n",
    "counts=pd.DataFrame(vector.toarray(),columns=vectorizer.get_feature_names())\n",
    "print(\"(k) 2 favorite words and their log-prob\")\n",
    "#counts\n",
    "#show the top 10 most common words\n",
    "#print(counts.T.sort_values(by=0,ascending=False).head(10))\n",
    "import math\n",
    "count_love=counts['love'].sum()\n",
    "count_love_smooth=count_love+0.0001\n",
    "print(\"frequency of 'love':\"+ str(count_love_smooth))\n",
    "#print(sum_total)\n",
    "sum_total_smooth=sum_total+0.0001*v\n",
    "#print(sum_total_smooth)\n",
    "log_prob_love=math.log(count_love_smooth/sum_total_smooth)\n",
    "print(\"The log-prob of the 'love' favorite word is :\"+ str(log_prob_love))\n",
    "count_key=counts['key'].sum()\n",
    "count_key_smooth=count_key+0.0001\n",
    "print(\"frequency of 'key':\"+str(count_key_smooth))\n",
    "log_prob_key=math.log(count_key_smooth/sum_total_smooth)\n",
    "print(\"The log-prob of the 'key'favorite word is :\"+str(log_prob_free))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f8fce6691405556ffb3062e1861de211962c440fc65c8599586c6206ef07fdf2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
