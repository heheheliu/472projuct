{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdbb26b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: C:\\Users\\15145\\anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - gensim\n",
      "\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  python_abi         conda-forge/win-64::python_abi-3.8-2_cp38\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  conda              pkgs/main::conda-4.11.0-py38haa95532_0 --> conda-forge::conda-4.11.0-py38haa244fe_0\n",
      "\n",
      "\n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a33d46bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: C:\\Users\\15145\\anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - conda\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    conda-4.11.0               |   py38haa95532_0        14.4 MB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        14.4 MB\n",
      "\n",
      "The following packages will be REMOVED:\n",
      "\n",
      "  python_abi-3.8-2_cp38\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  conda              conda-forge::conda-4.11.0-py38haa244f~ --> pkgs/main::conda-4.11.0-py38haa95532_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "conda-4.11.0         | 14.4 MB   |            |   0% \n",
      "conda-4.11.0         | 14.4 MB   |            |   0% \n",
      "conda-4.11.0         | 14.4 MB   | 3          |   3% \n",
      "conda-4.11.0         | 14.4 MB   | 4          |   5% \n",
      "conda-4.11.0         | 14.4 MB   | 9          |   9% \n",
      "conda-4.11.0         | 14.4 MB   | #4         |  14% \n",
      "conda-4.11.0         | 14.4 MB   | #7         |  17% \n",
      "conda-4.11.0         | 14.4 MB   | ##         |  20% \n",
      "conda-4.11.0         | 14.4 MB   | ##3        |  23% \n",
      "conda-4.11.0         | 14.4 MB   | ##6        |  26% \n",
      "conda-4.11.0         | 14.4 MB   | ##9        |  29% \n",
      "conda-4.11.0         | 14.4 MB   | ###2       |  32% \n",
      "conda-4.11.0         | 14.4 MB   | ###5       |  35% \n",
      "conda-4.11.0         | 14.4 MB   | ###8       |  38% \n",
      "conda-4.11.0         | 14.4 MB   | ####1      |  41% \n",
      "conda-4.11.0         | 14.4 MB   | ####4      |  44% \n",
      "conda-4.11.0         | 14.4 MB   | ####7      |  47% \n",
      "conda-4.11.0         | 14.4 MB   | #####      |  50% \n",
      "conda-4.11.0         | 14.4 MB   | #####3     |  53% \n",
      "conda-4.11.0         | 14.4 MB   | #####5     |  56% \n",
      "conda-4.11.0         | 14.4 MB   | #####8     |  59% \n",
      "conda-4.11.0         | 14.4 MB   | ######1    |  62% \n",
      "conda-4.11.0         | 14.4 MB   | ######4    |  64% \n",
      "conda-4.11.0         | 14.4 MB   | ######7    |  68% \n",
      "conda-4.11.0         | 14.4 MB   | #######    |  71% \n",
      "conda-4.11.0         | 14.4 MB   | #######3   |  73% \n",
      "conda-4.11.0         | 14.4 MB   | #######6   |  76% \n",
      "conda-4.11.0         | 14.4 MB   | #######9   |  79% \n",
      "conda-4.11.0         | 14.4 MB   | ########1  |  82% \n",
      "conda-4.11.0         | 14.4 MB   | ########5  |  85% \n",
      "conda-4.11.0         | 14.4 MB   | ########8  |  88% \n",
      "conda-4.11.0         | 14.4 MB   | #########  |  91% \n",
      "conda-4.11.0         | 14.4 MB   | #########3 |  94% \n",
      "conda-4.11.0         | 14.4 MB   | #########6 |  97% \n",
      "conda-4.11.0         | 14.4 MB   | #########9 |  99% \n",
      "conda-4.11.0         | 14.4 MB   | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda update -n base -c defaults conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3aa9e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader\n",
    "#show all available models in gensim-data\n",
    "print(list(gensim.downloader.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91d57b5a",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 3.35 GiB for an array with shape (3000000, 300) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-c90105e124b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgoogle_300\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownloader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'word2vec-google-news-300'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#google 300 words embedding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# task 2.1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mwiki_100\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownloader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'glove-wiki-gigaword-100'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m# 2 new models form different corpora\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\downloader.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, return_path)\u001b[0m\n\u001b[0;32m    501\u001b[0m         \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBASE_DIR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    502\u001b[0m         \u001b[0mmodule\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__import__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 503\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    504\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~/gensim-data\\word2vec-google-news-300\\__init__.py\u001b[0m in \u001b[0;36mload_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'word2vec-google-news-300'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"word2vec-google-news-300.gz\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[0;32m   1628\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1629\u001b[0m         \"\"\"\n\u001b[1;32m-> 1630\u001b[1;33m         return _load_word2vec_format(\n\u001b[0m\u001b[0;32m   1631\u001b[0m             \u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1632\u001b[0m             \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mno_header\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mno_header\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[0;32m   1904\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1905\u001b[0m             \u001b[0mvocab_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1906\u001b[1;33m         \u001b[0mkv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1907\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1908\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, vector_size, count, dtype, mapfile_path)\u001b[0m\n\u001b[0;32m    225\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey_to_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 227\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# formerly known as syn0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    228\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 3.35 GiB for an array with shape (3000000, 300) and data type float32"
     ]
    }
   ],
   "source": [
    "google_300=gensim.downloader.load('word2vec-google-news-300')#google 300 words embedding\n",
    "\n",
    "\n",
    "# task 2.1\n",
    "wiki_100=gensim.downloader.load('glove-wiki-gigaword-100')# 2 new models form different corpora \n",
    "twitter_100=gensim.downloader.load('glove-twitter-100')#but with same embedding size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81745c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['question', 'answer', '0', '1', '2', '3'], ['enormously', 'tremendously', 'appropriately', 'uniquely', 'tremendously', 'decidedly'], ['provisions', 'stipulations', 'stipulations', 'interrelations', 'jurisdictions', 'interpretations'], ['haphazardly', 'randomly', 'dangerously', 'densely', 'randomly', 'linearly'], ['prominent', 'conspicuous', 'battered', 'ancient', 'mysterious', 'conspicuous'], ['zenith', 'pinnacle', 'completion', 'pinnacle', 'outset', 'decline'], ['flawed', 'imperfect', 'tiny', 'imperfect', 'lustrous', 'crude'], ['urgently', 'desperately', 'typically', 'conceivably', 'tentatively', 'desperately'], ['consumed', 'eaten', 'bred', 'caught', 'eaten', 'supplied'], ['advent', 'coming', 'coming', 'arrest', 'financing', 'stability'], ['concisely', 'succinctly', 'powerfully', 'positively', 'freely', 'succinctly'], ['salutes', 'greetings', 'information', 'ceremonies', 'greetings', 'privileges'], ['solitary', 'alone', 'alert', 'restless', 'alone', 'fearless'], ['hasten', 'accelerate', 'permit', 'determine', 'accelerate', 'accompany'], ['perseverance', 'endurance', 'endurance', 'skill', 'generosity', 'disturbance'], ['fanciful', 'imaginative', 'familiar', 'imaginative', 'apparent', 'logical'], ['showed', 'demonstrated', 'demonstrated', 'published', 'repeated', 'postponed'], ['constantly', 'continually', 'instantly', 'continually', 'rapidly', 'accidentally'], ['issues', 'subjects', 'training', 'salaries', 'subjects', 'benefits'], ['furnish', 'supply', 'supply', 'impress', 'protect', 'advise'], ['costly', 'expensive', 'expensive', 'beautiful', 'popular', 'complicated'], ['recognized', 'acknowledged', 'successful', 'depicted', 'acknowledged', 'welcomed'], ['spot', 'location', 'climate', 'latitude', 'sea', 'location'], ['make', 'earn', 'earn', 'print', 'trade', 'borrow'], ['often', 'frequently', 'definitely', 'frequently', 'chemically', 'hardly'], ['easygoing', 'relaxed', 'frontier', 'boring', 'farming', 'relaxed'], ['debate', 'argument', 'war', 'argument', 'election', 'competition'], ['narrow', 'thin', 'clear', 'freezing', 'thin', 'poisonous'], ['arranged', 'planned', 'planned', 'explained', 'studied', 'discarded'], ['infinite', 'limitless', 'limitless', 'relative', 'unusual', 'structural'], ['showy', 'striking', 'striking', 'prickly', 'entertaining', 'incidental'], ['levied', 'imposed', 'imposed', 'believed', 'requested', 'correlated'], ['deftly', 'skillfully', 'skillfully', 'prudently', 'occasionally', 'humorously'], ['distribute', 'circulate', 'commercialize', 'circulate', 'research', 'acknowledge'], ['discrepancies', 'differences', 'weights', 'deposits', 'wavelengths', 'differences'], ['prolific', 'productive', 'productive', 'serious', 'capable', 'promising'], ['unmatched', 'unequaled', 'unrecognized', 'unequaled', 'alienated', 'emulated'], ['peculiarly', 'uniquely', 'partly', 'uniquely', 'patriotically', 'suspiciously'], ['hue', 'color', 'glare', 'contrast', 'color', 'scent'], ['hind', 'rear', 'curved', 'muscular', 'hairy', 'rear'], ['highlight', 'accentuate', 'alter', 'imitate', 'accentuate', 'restore'], ['hastily', 'hurriedly', 'hurriedly', 'shrewdly', 'habitually', 'chronologically'], ['temperate', 'mild', 'cold', 'mild', 'short', 'windy'], ['grin', 'smile', 'exercise', 'rest', 'joke', 'smile'], ['verbally', 'orally', 'orally', 'overtly', 'fittingly', 'verbosely'], ['physician', 'doctor', 'chemist', 'pharmacist', 'nurse', 'doctor'], ['essentially', 'basically', 'possibly', 'eagerly', 'basically', 'ordinarily'], ['keen', 'sharp', 'useful', 'simple', 'famous', 'sharp'], ['situated', 'positioned', 'rotating', 'isolated', 'emptying', 'positioned'], ['principal', 'major', 'most', 'numerous', 'major', 'exceptional'], ['slowly', 'gradually', 'rarely', 'gradually', 'effectively', 'continuously'], ['built', 'constructed', 'constructed', 'proposed', 'financed', 'organized'], ['tasks', 'jobs', 'customers', 'materials', 'shops', 'jobs'], ['unlikely', 'improbable', 'improbable', 'disagreeable', 'different', 'unpopular'], ['halfheartedly', 'apathetically', 'customarily', 'bipartisanly', 'apathetically', 'unconventionally'], ['annals', 'chronicles', 'homes', 'trails', 'chronicles', 'songs'], ['wildly', 'furiously', 'distinctively', 'mysteriously', 'abruptly', 'furiously'], ['hailed', 'acclaimed', 'judged', 'acclaimed', 'remembered', 'addressed'], ['command', 'mastery', 'observation', 'love', 'awareness', 'mastery'], ['concocted', 'devised', 'devised', 'cleaned', 'requested', 'supervised'], ['prospective', 'potential', 'particular', 'prudent', 'potential', 'prominent'], ['generally', 'broadly', 'descriptively', 'broadly', 'controversially', 'accurately'], ['sustained', 'prolonged', 'prolonged', 'refined', 'lowered', 'analyzed'], ['perilous', 'dangerous', 'binding', 'exciting', 'offensive', 'dangerous'], ['tranquillity', 'peacefulness', 'peacefulness', 'harshness', 'weariness', 'happiness'], ['dissipate', 'disperse', 'disperse', 'isolate', 'disguise', 'photograph'], ['primarily', 'chiefly', 'occasionally', 'cautiously', 'consistently', 'chiefly'], ['colloquial', 'conversational', 'recorded', 'misunderstood', 'incorrect', 'conversational'], ['resolved', 'settled', 'publicized', 'forgotten', 'settled', 'examined'], ['feasible', 'possible', 'permitted', 'possible', 'equitable', 'evident'], ['expeditiously', 'rapidly', 'frequently', 'actually', 'rapidly', 'repeatedly'], ['percentage', 'proportion', 'volume', 'sample', 'proportion', 'profit'], ['terminated', 'ended', 'ended', 'posed', 'postponed', 'evaluated'], ['uniform', 'alike', 'hard', 'complex', 'alike', 'sharp'], ['figure', 'solve', 'list', 'solve', 'divide', 'express'], ['sufficient', 'enough', 'recent', 'physiological', 'enough', 'valuable'], ['fashion', 'manner', 'ration', 'fathom', 'craze', 'manner'], ['marketed', 'sold', 'frozen', 'sold', 'sweetened', 'diluted'], ['bigger', 'larger', 'steadier', 'closer', 'larger', 'better'], ['roots', 'origins', 'origins', 'rituals', 'cure', 'function'], ['normally', 'ordinarily', 'haltingly', 'ordinarily', 'permanently', 'periodically']]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "with open('synonyms.csv','r') as synonyms_file:\n",
    "    read=csv.reader(synonyms_file)\n",
    "    read_file=list(read)\n",
    "    \n",
    "    print(read_file[3][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65196f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import reader\n",
    "\n",
    "analysis = []\n",
    "correct = 0\n",
    "guessed = 0\n",
    "with open(\"synonyms.csv\", \"r\") as f:\n",
    "    read = reader(f)\n",
    "    reader = list(read)\n",
    "    for i in range(1,len(reader)):\n",
    "        max_similarity = 0\n",
    "        guess = reader[i][2]\n",
    "        label = \"correct\"\n",
    "        if reader[i][0] in google_300 and ( reader[i][2] in google_300 or reader[i][3] in google_300 or reader[i][4] in google_300 or reader[i][5] in google_300):\n",
    "            label = \"correct\"\n",
    "            correct += 1\n",
    "        else:\n",
    "            label = \"guess\"\n",
    "            guessed += 1\n",
    "        for j in range(2,len(reader[i])):\n",
    "            try:\n",
    "                if max_similarity < google_300.similarity(reader[i][0], reader[i][j]):\n",
    "                    guess = reader[i][j]\n",
    "                    max_similarity = google_300.similarity(reader[i][0], reader[i][j])\n",
    "                    \n",
    "            except:\n",
    "                pass\n",
    "        if (label !=\"guess\" and guess != reader[i][1]):\n",
    "            label = \"wrong\"\n",
    "            correct -= 1\n",
    "        analysis.append([reader[i][0]+\",\"+reader[i][1]+\",\"+guess+\",\"+label])     \n",
    "with open(\"word2vec-google-news-300-details.csv\", 'w', newline='') as myfile:\n",
    "     wr = csv.writer(myfile)\n",
    "     wr.writerows(analysis)\n",
    " \n",
    "                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9f73c66",
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'analysis.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-5627ffd0197a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtotal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mguessed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"word2vec-google-news-300\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"analysis.csv\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'w'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0manalysisfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mwr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manalysisfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mwr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriterows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'analysis.csv'"
     ]
    }
   ],
   "source": [
    "corpus_google300 = len(google_300)\n",
    "accuracy_google300 = correct_google300/(80-guessed)\n",
    "total_google300 = 80-guessed\n",
    "\n",
    "row1 = \"word2vec-google-news-300\"+\",\"+str(corpus_google300)+\",\"+str(correct_google300)+\",\"+str(total_google300)+\",\"+str(accuracy_google300)\n",
    "with open(\"analysis.csv\", 'w', newline='') as analysisfile:\n",
    "     wr = csv.writer(analysisfile)\n",
    "     wr.writerows([[row1]])\n",
    "     analysisfile.write('n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "852efb36",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-d5e7b862e4fb>, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-d5e7b862e4fb>\"\u001b[1;36m, line \u001b[1;32m13\u001b[0m\n\u001b[1;33m    if reader[i][0] in twitter_25 and (reader[i][2] in twitter_25 or reader[i][3] in twitter_25 reader[i][4] in twitter_25 or reader[i][5] in twitter_25):\u001b[0m\n\u001b[1;37m                                                                                                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from csv import reader\n",
    "\n",
    "analysis = []\n",
    "correct = 0\n",
    "guessed = 0\n",
    "with open(\"synonyms.csv\", \"r\") as f:\n",
    "    read = reader(f)\n",
    "    reader = list(read)\n",
    "    for i in range(1,len(reader)):\n",
    "        max_similarity = 0\n",
    "        guess = reader[i][2]\n",
    "        label = \"correct\"\n",
    "        if reader[i][0] in wiki_100 and ( reader[i][2] in wiki_100 or reader[i][3] in wiki_100 or reader[i][4] in wiki_100 or reader[i][5] in wiki_100):\n",
    "            label = \"correct\"\n",
    "            correct += 1\n",
    "        else:\n",
    "            label = \"guess\"\n",
    "            guessed += 1\n",
    "        for j in range(2,len(reader[i])):\n",
    "            try:\n",
    "                if max_similarity < wiki_100.similarity(reader[i][0], reader[i][j]):\n",
    "                    guess = reader[i][j]\n",
    "                    max_similarity = wiki_100.similarity(reader[i][0], reader[i][j])\n",
    "                    \n",
    "            except:\n",
    "                pass\n",
    "        if (label !=\"guess\" and guess != reader[i][1]):\n",
    "            label = \"wrong\"\n",
    "            correct -= 1\n",
    "        analysis.append([reader[i][0]+\",\"+reader[i][1]+\",\"+guess+\",\"+label])     \n",
    "with open(\"glove-wiki-gigaword-100.csv\", 'w', newline='') as myfile:\n",
    "     wr = csv.writer(myfile)\n",
    "     wr.writerows(analysis)\n",
    "\n",
    "\n",
    "corpus = len(wiki_100)\n",
    "accuracy = correct/(80-guessed)\n",
    "total = 80-guessed\n",
    "\n",
    "row = \"glove-wiki-gigaword-100\"+\",\"+str(corpus)+\",\"+str(correct)+\",\"+str(total)+\",\"+str(accuracy)\n",
    "with open(\"wiki100analysis.csv\", 'w', newline='') as analysisfile:\n",
    "     wr = csv.writer(analysisfile)\n",
    "     wr.writerows([[row]])\n",
    "     \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9ae56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import reader\n",
    "\n",
    "analysis = []\n",
    "correct = 0\n",
    "guessed = 0\n",
    "with open(\"synonyms.csv\", \"r\") as f:\n",
    "    read = reader(f)\n",
    "    reader = list(read)\n",
    "    for i in range(1,len(reader)):\n",
    "        max_similarity = 0\n",
    "        guess = reader[i][2]\n",
    "        label = \"correct\"\n",
    "        if reader[i][0] in twitter_100 and ( reader[i][2] in twitter_100 or reader[i][3] in twitter_100 or reader[i][4] in twitter_100 or reader[i][5] in twitter_100):\n",
    "            label = \"correct\"\n",
    "            correct += 1\n",
    "        else:\n",
    "            label = \"guess\"\n",
    "            guessed += 1\n",
    "        for j in range(2,len(reader[i])):\n",
    "            try:\n",
    "                if max_similarity < twitter_100.similarity(reader[i][0], reader[i][j]):\n",
    "                    guess = reader[i][j]\n",
    "                    max_similarity = twitter_100.similarity(reader[i][0], reader[i][j])\n",
    "                    \n",
    "            except:\n",
    "                pass\n",
    "        if (label !=\"guess\" and guess != reader[i][1]):\n",
    "            label = \"wrong\"\n",
    "            correct -= 1\n",
    "        analysis.append([reader[i][0]+\",\"+reader[i][1]+\",\"+guess+\",\"+label])     \n",
    "with open(\"glove-twitter-100.csv\", 'w', newline='') as myfile:\n",
    "     wr = csv.writer(myfile)\n",
    "     wr.writerows(analysis)\n",
    "\n",
    "\n",
    "corpus = len(twitter_100)\n",
    "accuracy = correct/(80-guessed)\n",
    "total = 80-guessed\n",
    "row = \"glove-twitter-100\"+\",\"+str(corpus)+\",\"+str(correct)+\",\"+str(total)+\",\"+str(accuracy)\n",
    "with open(\"twitter100analysis.csv\", 'w', newline='') as analysisfile:\n",
    "     wr = csv.writer(analysisfile)\n",
    "     wr.writerows([[row]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71fb2c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 199.5/199.5MB downloaded\n"
     ]
    }
   ],
   "source": [
    "# task 2.2\n",
    "twitter_25=gensim.downloader.load('glove-twitter-25')#2 new models from the same corpus but different embedding size\n",
    "twitter_50=gensim.downloader.load('glove-twitter-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4df9e2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import reader\n",
    "\n",
    "analysis = []\n",
    "correct = 0\n",
    "guessed = 0\n",
    "with open(\"synonyms.csv\", \"r\") as f:\n",
    "    read = reader(f)\n",
    "    reader = list(read)\n",
    "    for i in range(1,len(reader)):\n",
    "        max_similarity = 0\n",
    "        guess = reader[i][2]\n",
    "        label = \"correct\"\n",
    "        if reader[i][0] in twitter_25 and ( reader[i][2] in twitter_25 or reader[i][3] in twitter_25 or reader[i][4] in twitter_25 or reader[i][5] in twitter_25):\n",
    "            label = \"correct\"\n",
    "            correct += 1\n",
    "        else:\n",
    "            label = \"guess\"\n",
    "            guessed += 1\n",
    "        for j in range(2,len(reader[i])):\n",
    "            try:\n",
    "                if max_similarity < twitter_25.similarity(reader[i][0], reader[i][j]):\n",
    "                    guess = reader[i][j]\n",
    "                    max_similarity = twitter_25.similarity(reader[i][0], reader[i][j])\n",
    "                    \n",
    "            except:\n",
    "                pass\n",
    "        if (label !=\"guess\" and guess != reader[i][1]):\n",
    "            label = \"wrong\"\n",
    "            correct -= 1\n",
    "        analysis.append([reader[i][0]+\",\"+reader[i][1]+\",\"+guess+\",\"+label])     \n",
    "with open(\"glove-twitter-25.csv\", 'w', newline='') as myfile:\n",
    "     wr = csv.writer(myfile)\n",
    "     wr.writerows(analysis)\n",
    "    \n",
    "    \n",
    "\n",
    "corpus = len(twitter_25)\n",
    "accuracy = correct/(80-guessed)\n",
    "total = 80-guessed\n",
    "row = \"glove-twitter-25\"+\",\"+str(corpus)+\",\"+str(correct)+\",\"+str(total)+\",\"+str(accuracy)\n",
    "with open(\"twitter25analysis.csv\", 'w', newline='') as analysisfile:\n",
    "     wr = csv.writer(analysisfile)\n",
    "     wr.writerows([[row]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c1d3885",
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import reader\n",
    "\n",
    "analysis = []\n",
    "correct = 0\n",
    "guessed = 0\n",
    "with open(\"synonyms.csv\", \"r\") as f:\n",
    "    read = reader(f)\n",
    "    reader = list(read)\n",
    "    for i in range(1,len(reader)):\n",
    "        max_similarity = 0\n",
    "        guess = reader[i][2]\n",
    "        label = \"correct\"\n",
    "        if reader[i][0] in twitter_50 and ( reader[i][2] in twitter_50 or reader[i][3] in twitter_50 or reader[i][4] in twitter_50 or reader[i][5] in twitter_50):\n",
    "            label = \"correct\"\n",
    "            correct += 1\n",
    "        else:\n",
    "            label = \"guess\"\n",
    "            guessed += 1\n",
    "        for j in range(2,len(reader[i])):\n",
    "            try:\n",
    "                if max_similarity < twitter_50.similarity(reader[i][0], reader[i][j]):\n",
    "                    guess = reader[i][j]\n",
    "                    max_similarity = twitter_50.similarity(reader[i][0], reader[i][j])\n",
    "                    \n",
    "            except:\n",
    "                pass\n",
    "        if (label !=\"guess\" and guess != reader[i][1]):\n",
    "            label = \"wrong\"\n",
    "            correct -= 1\n",
    "        analysis.append([reader[i][0]+\",\"+reader[i][1]+\",\"+guess+\",\"+label])     \n",
    "with open(\"glove-twitter-50.csv\", 'w', newline='') as myfile:\n",
    "     wr = csv.writer(myfile)\n",
    "     wr.writerows(analysis)\n",
    "    \n",
    "    \n",
    "\n",
    "corpus = len(twitter_50)\n",
    "accuracy = correct/(80-guessed)\n",
    "total = 80-guessed\n",
    "row = \"glove-twitter-50\"+\",\"+str(corpus)+\",\"+str(correct)+\",\"+str(total)+\",\"+str(accuracy)\n",
    "with open(\"twitter50analysis.csv\", 'w', newline='') as analysisfile:\n",
    "     wr = csv.writer(analysisfile)\n",
    "     wr.writerows([[row]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d9a9cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
